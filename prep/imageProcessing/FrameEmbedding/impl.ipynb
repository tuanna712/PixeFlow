{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374e1a52",
   "metadata": {},
   "source": [
    "## CLIP - Image Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3683d4b",
   "metadata": {},
   "source": [
    "#### 1. Download model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc8e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os, shutil\n",
    "from pathlib import Path\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "sys.path.append(str(Path('impl.ipynb').resolve().parents[3]))\n",
    "from prep.params import SQLITE_PATH, CLIP_DIR\n",
    "\n",
    "# Define the directory where models will be saved\n",
    "TEMP_DIR = \"./.clip_model\"\n",
    "# Define the specific CLIP model ID from Hugging Face\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.makedirs(TEMP_DIR)\n",
    "\n",
    "# Load the model and processor from Hugging Face\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Save the model and processor to the specified local directory\n",
    "processor.save_pretrained(\"../../\"+CLIP_DIR)\n",
    "model.save_pretrained(\"../../\"+CLIP_DIR)\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(TEMP_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a46e9",
   "metadata": {},
   "source": [
    "#### 2. Load model from local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745cd2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dna-tuananguyen/anaconda3/envs/apitcdk/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys, os, shutil\n",
    "from pathlib import Path\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "sys.path.append(str(Path('impl.ipynb').resolve().parents[3]))\n",
    "from prep.params import CLIP_DIR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(\"../../\"+CLIP_DIR)\n",
    "model = CLIPModel.from_pretrained(\"../../\"+CLIP_DIR)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bbf67",
   "metadata": {},
   "source": [
    "#### 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec347073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image embedding shape: torch.Size([1, 512])\n",
      "Image embedding (first 5 values): [-0.00978781282901764, 0.01276974193751812, -0.02741880528628826, 0.001967571210116148, -0.00593261793255806]\n",
      "\n",
      "Text embeddings shape: torch.Size([3, 512])\n",
      "Text embedding for 'a photo of 2 cats' (first 5 values): [0.031352195888757706, 0.0010832290863618255, -0.06258819997310638, -0.037271205335855484, 0.008031794801354408]\n",
      "Text embedding for 'a picture of a pink couch' (first 5 values): [0.0009097328293137252, -0.0035542109981179237, -0.020510952919721603, -0.05030852556228638, -0.02737904153764248]\n",
      "Text embedding for 'a photo of 2 remote controls' (first 5 values): [-0.003358860034495592, 0.002799508860334754, -0.08756726235151291, 0.007565741427242756, 0.01846543326973915]\n",
      "\n",
      "Image-Text similarity logits (before softmax): tensor([[27.5308, 23.8256, 27.3588]], device='mps:0')\n",
      "Image-Text similarity probabilities (softmax): tensor([[0.5357, 0.0132, 0.4511]], device='mps:0')\n",
      "Probability for 'a photo of 2 cats': 0.5357\n",
      "Probability for 'a picture of a pink couch': 0.0132\n",
      "Probability for 'a photo of 2 remote controls': 0.4511\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Added more text for better context\n",
    "text_inputs = [\"a photo of 2 cats\", \"a picture of a pink couch\", \"a photo of 2 remote controls\"]\n",
    "\n",
    "inputs = processor(\n",
    "                    text=text_inputs,\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                   ).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "# Image embedding\n",
    "image_embedding = outputs.image_embeds\n",
    "print(f\"\\nImage embedding shape: {image_embedding.shape}\")\n",
    "print(f\"Image embedding (first 5 values): {image_embedding[0, :5].tolist()}\")\n",
    "\n",
    "\n",
    "# Text embeddings\n",
    "# This tensor contains the embeddings for each of the input text phrases.\n",
    "text_embeddings = outputs.text_embeds\n",
    "print(f\"\\nText embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Text embedding for '{text_inputs[0]}' (first 5 values): {text_embeddings[0, :5].tolist()}\")\n",
    "print(f\"Text embedding for '{text_inputs[1]}' (first 5 values): {text_embeddings[1, :5].tolist()}\")\n",
    "if len(text_inputs) > 2:\n",
    "    print(f\"Text embedding for '{text_inputs[2]}' (first 5 values): {text_embeddings[2, :5].tolist()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The original similarity calculation ---\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "\n",
    "print(f\"\\nImage-Text similarity logits (before softmax): {logits_per_image}\")\n",
    "print(f\"Image-Text similarity probabilities (softmax): {probs}\")\n",
    "\n",
    "# To interpret the probabilities:\n",
    "for i, prob in enumerate(probs[0]): # Assuming one image, iterate over text probabilities\n",
    "    print(f\"Probability for '{text_inputs[i]}': {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09db8c",
   "metadata": {},
   "source": [
    "### 4. Load from packed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580bb8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08948638  0.15628737 -0.35676813 -0.19910409 -0.05353561 -0.23494473\n",
      " -0.36256215  0.13674879  0.245684    0.01422149]\n",
      "[ 0.3023144   0.01044509 -0.60350806 -0.35938838  0.07744727  0.21590227\n",
      " -0.24014613 -0.69491875 -0.38131976  0.04199659]\n"
     ]
    }
   ],
   "source": [
    "import requests, sys\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('impl.ipynb').resolve().parents[3]))\n",
    "from FrameEmb import ClipEncoder\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Image to base64\n",
    "import base64\n",
    "from io import BytesIO\n",
    "def image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "image_base64 = image_to_base64(image)\n",
    "\n",
    "CE = ClipEncoder()\n",
    "# Ensure the output tensor is on the correct device\n",
    "result = CE.image_encode(image_base64)\n",
    "print(result[:10])\n",
    "\n",
    "text_inputs = \"a photo of 2 cats\"\n",
    "result = CE.text_encode(text_inputs)\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85fd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apitcdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
